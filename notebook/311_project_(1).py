# -*- coding: utf-8 -*-
"""311 project (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IBHQ8TjDA6UKhuow0J_rR_yl-LZhBkWf
"""

import pandas as pd
import numpy as np
import time
import matplotlib.pyplot as plt
import seaborn as sns 
from scipy.stats import f_oneway

from google.colab import drive
drive.mount('/content/drive')

"""* Import a 311 NYC service request."""

hr1=pd.read_csv("/content/drive/MyDrive/simplilearn/python with data science /project1/311_Service_Requests_from_2010_to_Present.csv")

hr1.head()

hr1.describe()

hr1.info()

hr1.dtypes

"""*first we see what % of missing data in dataset

and ploting a values in bar chart
"""

hr1.isnull().sum()/len(hr1)*100

"""to visualize number of null values in dataset 
by ploting bar chart we can see 
which column has what num of null values 
"""

hr1.isnull().sum().plot(kind='bar', figsize=(10,5),title = 'missing values')

"""as visible in bar graph many columns has max missing values that contant null

second task is to remove not columns  having maximum null values.
"""

hr1.keys()

un_useble= ['Agency Name','Incident Address','Street Name','Cross Street 1','Cross Street 2','Intersection Street 1',
'Intersection Street 2','Address Type','Park Facility Name','Park Borough','School Name',
'School Number','School Region','School Code','School Phone Number','School Address','School City',
'School State','School Zip','School Not Found','School or Citywide Complaint','Vehicle Type',
'Taxi Company Borough','Taxi Pick Up Location','Bridge Highway Name','Bridge Highway Direction',
'Road Ramp','Bridge Highway Segment','Garage Lot Name','Ferry Direction','Ferry Terminal Name','Landmark',
'X Coordinate (State Plane)','Y Coordinate (State Plane)','Due Date','Resolution Action Updated Date','Community Board','Facility Type']

"""serching a values in status column and visualize what num of tipe values related to which cattegiry by bar chart 

"""

hr1['Status'].value_counts().plot(kind='bar',alpha=0.6,figsize=(6,10))
plt.show()

hr1.drop(un_useble, inplace=True, axis=1)
hr1= hr1[(hr1['Latitude'].notnull())& (hr1['Longitude'].notnull()) & (hr1['Descriptor'].notnull())]
hr1 = hr1[hr1['Status']=='Closed']
hr1.drop(['Status'],inplace=True, axis=1)
hr1.info()

"""second task 
changing data type from object to date and time by using date and time module 

"""

hr1["Created Date"]=pd.to_datetime(hr1['Created Date'])
hr1["Closed Date"]=pd.to_datetime(hr1['Closed Date'])

hr1.info()

""" add new calumn "Request_closing_time" for colepsed time between created date anf closed date 

"""

hr1['Request_closing_time']=hr1["Closed Date"]-hr1["Created Date"]

hr1['Request_closing_time']

hr1.info()

hr1.columns

"""then again see the % of null values remain in data set """

hr1.isnull().sum()/len(hr1)*100

"""complain distribution across borough

visualizing in pie chart
"""

hr1['Borough'].value_counts()

colors = ['#639ace','#ca6b39','#7f67ca','#5ba85f','#c360aa','#a7993f','#cc566a']
hr1['Borough'].value_counts().plot(kind='pie',autopct='%1.1f%%',
                        explode = (0.15, 0, 0, 0,0), startangle=45, shadow=False, colors = colors,
                        figsize = (8,6))
#plt.legend(title='BOROUGH', loc='upper right', bbox_to_anchor=(1.5,1))
plt.axis('equal')
plt.title('# complaints distribution across Boroughs (2015)\n')
plt.tight_layout()
plt.show()

hr1['Request_closing_time'].sort_values()

"""to calculating avarage time  we have to cal aprox time for each values of time

and view values of request closing time  in hour (aprox)
"""

hr1['Request_Closing_Hours'] = hr1['Request_closing_time'].astype('timedelta64[h]')+1
hr1[['Request_closing_time','Request_Closing_Hours']].head()

"""#grouping complaint type and borough based on Request Closing Hour

#and taking a visualized look of the data--based on perticular location what type and number of compl. accur at
"""

grouped_data = hr1.groupby(['Complaint Type','Borough'])[['Request_Closing_Hours']].mean().unstack()

grouped_data.head()

"""#visualizing top 5 complaints in each borough using subplots"""

col_number = 2
row_number = 3
fig, axes = plt.subplots(row_number,col_number, figsize=(12,8))

for i, (label,col) in enumerate(grouped_data.iteritems()):
    ax = axes[int(i/col_number), i%col_number]
    col = col.sort_values(ascending=True)[:15]
    col.plot(kind='barh', ax=ax)
    ax.set_title(label)
    
plt.tight_layout()

(hr1['Complaint Type'].value_counts()).head(25).plot(kind='bar',
                                                    figsize=(10,6),title = 'Most Frequent Complaints in Brooklyn')

"""#doing ANOVA test to check whether the average response time across complaint types is similar or not

**h0**= average response time across complaint types is similar
if p>0.05

ha=average response time across complaint types is not  similar 
so for proove that we have cheake p value p<0.05
"""

data = {}
for complaint in hr1['Complaint Type'].unique():
    data[complaint] = np.log(hr1[hr1['Complaint Type']==complaint]['Request_Closing_Hours'])

data[complaint].head()

data.keys()

# import f_oneway from scipy.stats library

stat, p = f_oneway(data['Noise - Street/Sidewalk'],data['Blocked Driveway'],data['Illegal Parking'],data['Derelict Vehicle'],
                   data['Noise - Commercial'])
print('Statistics=%.3f, p=%.3f' % (stat, p))

if p > 0.05:
    print('the average response time across complaint types is similar hence "fail to reject H0"')
else:
    print('the average response time across complaint types is not similar hence "reject H0"')

"""checking correlation between location and complaint types

to performe corelation test we have all the value in numerical formate so first task is to change cattegorical values to numerical 
by using getdummies ()

then perform a test between location and comp. type
"""

corr_test_data = hr1[['Complaint Type','Borough','Longitude','Latitude','City']]

corr_test_data['Complaint Type']=pd.get_dummies(corr_test_data['Complaint Type'])
corr_test_data['Borough']=pd.get_dummies(corr_test_data['Borough'])
corr_test_data['City']=pd.get_dummies(corr_test_data['City'])

corr_test_data.corr()

"""view the correlation by using heatmap 
using seaborn lib 
"""

import seaborn as sns

ax = sns.heatmap(corr_test_data.corr())

